<!--
GENERATED FILE

README.md is generated from a template file, src/README.md.in, and JSON snippets under src/.  If you need to revise narrative text, edit src/README.md.in.  If you need to revise data, please find and revise the containing snippet.  Editing patterns follow the patterns described in the CASE website's CONTRIBUTE.md:
https://github.com/casework/casework.github.io/blob/master/CONTRIBUTE.md#maintenance-of-generated-files
-->


# HypothesisTestResult (a.k.a. AnalyticInference) Examples

This illustration includes cyber-investigation examples that require hypothessis testing and a resulting evidence-based opinion/conclusion. The sample JSON-LD provides *proposed* use of a HypothesisTestResult facet atached to an AnalyticResult object. 

This structure is an updated example of AnalyticInference based on the paper [Standardization of File Recovery Classification and Authentication](https://doi.org/10.1016/j.diin.2019.06.004) by Casey, Nelson, and Hyde.  This example is also related to examples in [Standardization of forming and expressing preliminary evaluative opinions on digital evidence](https://doi.org/10.1016/j.fsidi.2019.200888) by Casey.  The data tampering examples ("Wiping", "Mass deletion") are analytic results related to evidence tampering, with background provided in the DFRWS-EU 2020 presentation "[Expressing evaluative conclusions in cases involving tampering of digital evidence](https://dfrws.org/wp-content/uploads/2020/06/DFRWS-EU-2020-Expressing-evaluative-conclusions-in-cases-involving-tampering-of-digital-evidence.pdf)" by Bollé, Servida, Polewczyk, Souvignet and Casey.

When performing evidence-based hypothesis testing, the evidenceEvaluation value is assigned to the observations, not the hypothesis.

When making inferences on the basis of observed evidence, it is important to consider alternatives. Selecting a single hypothesis without consideration of alternatives increases risk of confirmation bias. When conducting a cyber-investigation, observed evidence is the result of an activity, not the activity itself. The observed evidence could have an alternative explanation than the most obvious or initially imagined one. Therefore, it is good practice to consider alternative hypotheses when evaluating observed evidence, including the opposing hypothesis.


## State of this illustration

This illustration has been written in support of a Change Proposal under design and discussion, and hence may change in response to the proposal's evolution.  New ontology concepts are in the file [`drafting.ttl`](drafting.ttl).

Notes on how to revise this page are in [`CONTRIBUTE.md`](CONTRIBUTE.md).

A total JSON-LD file of the data on this page, plus supplementary data needed to support queries, is available here: [`inference.json`](inference.json).


## Example - Recovery of a xlsx file

A digital forensic tools presents a non-allocated file named `contacts.xlsx` as recovered with associated metadata. However, the tool does automatically check whether the recovered content currently stored on disk is the actual original content of the file. Therefore, it is necessary to treat this tool output as a hypothesis, and also assess alternative hypotheses

* Hypothesis 1: The file is `Fully Recovered`
* Hypothesis 2: The file is `Partially Recovered`
* Hypothesis 3: Only `Name and Metadata Recovered`
* Hypothesis 4: Only `Name Recovered`

Further analysis finds that the data on disk presented by the tool is incompatible with the file metadata and expected content type (Microsoft Excel file).

```json
@INFERENCE_XLSX_AUTHENTICATION_JSON@
```


### Query - Most probable hypothesis

To ask whether the file is `Fully Recovered` is the wrong question because it does not consider alternatives and raises the risk of confirmation bias.

Rather, the question is "Which class/category of file recovery is more supported by the evidence?" In this example, the result of only `Name and Metadata Recovered` has the highest assigned probability versus the others.

@QUERY_XLSX_INFERENCES_MD@


## Example - Machine learning and image location

Consider an example of a photograph of the Washington monument from an angle at dusk (fading light). Two machine learning classifiers produce the following assertions:

* *ML1 Hypothesis*: The object in the photograph is the Washington Monument in Washington, DC.
* *ML2 Hypothesis*: The object in the photograph is Cleopatra's Needle in New York City.

Further forensic analysis is performed of the photograph and its metadata, which reveals that it was taken in Washington DC. Inference assigns a value (e.g., probability, strength) to the observations (photograph content) and analysis outputs (geolocation information) given each Hypothesis:

*AnalyticInference 1*: The observations and analysis results are exceedingly more likely given the assertion that the object in the photograph is the Washington Monument in Washington, DC, rather than Cleopatra's Needle in New York City. 

```json
@INFERENCE_PHOTOLOCATION_IDENTIFICATION_JSON@
```

However, a skeptic might argue that photographs can be faked or geolocation information can be spoofed, and might make the following Hypothesis and AnalyticInference:

* *Skeptic's Hypothesis*: The photograph is fake.

*Skeptic's AnalyticInference*: The observations and analysis results are equally probable given the assertion that the object in the photograph is the Washington Monument in Washington, DC or Cleopatra's Needle in New York City. 

```json
@INFERENCE_PHOTOLOCATION_SKEPTIC_JSON@
```

If there is no indication that the photograph was tampered with, a reasonable decision maker would favor the first hypothesis test result over the Skeptic's. Conversely, if further analysis reveals that the photograph was tampered with, the decision maker might give more consideration to the Skeptic's hypothesis. Furthermore, the first AnalyticInference could be updated to match the Skeptic's. Therefore, hypothesis test results can change as new information becomes available.


### Query - Differences in basis evidence

Two hypothesis test results (analytic inferences) are referenced above (`kb:analysisresult-C16CEAB2...` and `kb:analysisresult-5BCEDE9F...`).  What differences in basis evidence did the two have?

The following table is the results of [this query](src/query-select-skeptic-difference.sparql).  "Supports A" is support for only the initial analyst's finding, "Supports B" is support for only the skeptic's finding.

@QUERY_SELECT_SKEPTIC_DIFFERENCE_MD@



## Example - Wiping

*Observations*: Consider an example of a file named “abc.exe” that is a secure erase program which overwrites the content and name of selected files, rendering them unrecoverable. This program has the same hash value of the secure erase program called “sdelete.exe” which is freely available on the Internet. Observed file system patterns on the computer are compatible with  use of “sdelete.exe” (Name overwritten, Zeroed content). Dates in file system metadata of these files corresponding to dates of Registry entries (UserAssist) and Prefetch files recording use of “abc.exe” program.

* Hypothesis 1: File wiping occurred 
* Hypothesis 2: File wiping did not occur

The following hypothesis test results can be stated in words as the observations are exceedingly more probable in light of Hypothesis 1 (wiping), rather than Hypothesis 2 (no wiping).

```json
@INFERENCE_FILE_WIPING_JSON@
```

## Example - Mass Deletion

*Observations*: All of the previously existing directories that last contained files of interest are now in deleted state on the disks, but were recovered using forensic software. These directories are listed below along with their last accessed timestamp, which indicates when they changed to deleted state. These previously existing directories, and others under the same parent directory, all have last accessed dates on 17 April 2021. Entries in Recycle Bin for these files are dated 17 April 2021.

* Hypothesis 1: A folder and all its contents were deleted on 17 April 2021 
* Hypothesis 2: A folder and all its contents were not deleted on 17 April 2021

The following hypothesis test results can be stated in words as the observations are exceedingly more probable in light of Hypothesis 1 (targeted and deliberate deletion), rather than Hypothesis 2 (not targeted and deliberate deletion)

```json
@INFERENCE_MASS_DELETION_JSON@
```

